\documentclass[ChapterTOCs,krantz2]{krantz} % Use krantz2 for 7" x 10" trim size

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{epigraph}
%-----------------------------------------------------------------------------
% Special-purpose color definitions (dark enough to print OK in black and white)
\usepackage{color}
\usepackage{amsthm}

% A few colors to replace the defaults for certain link types
\definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
\definecolor{darkorange}{rgb}{.71,0.21,0.01}
\definecolor{darkgreen}{rgb}{.12,.54,.11}

%-----------------------------------------------------------------------------
% The hyperref package gives us a pdf with properly built
% internal navigation ('pdf bookmarks' for the table of contents,
% internal cross-reference links, web links for URLs, etc.)
%\usepackage{hyperref}

\usepackage{url}

%% Define a new 'leo' style for the package that will use a smaller font.
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
\makeatother
%% Now actually use the newly defined style.
\urlstyle{leo}

%\newcommand{\asterism}{{ \footnotesize \smash{% 
%    \raisebox{-.2ex}{% 
%      \setlength{\tabcolsep}{0.5pt}%
%      \begin{tabular}{@{}cc@{}}% 
%  \multicolumn2c*\\[-1.5ex] *&*% 
%\end{tabular}}}}}

\newcommand{\parasep}{\begin{center}*\hspace{6em}*\hspace{6em}*\end{center}}

\newcommand*{\threesim}{%
 \mathrel{\vcenter{\offinterlineskip
  \hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}\vskip-.35ex\hbox{$\sim$}}}}

\newcommand{\asterism}{\smash{%
  \raisebox{-.5ex}{%
    \setlength{\tabcolsep}{-.5pt}%
    \begin{tabular}{@{}cc@{}%
  \multicolumn2c*\\[-2ex]*&*%
\end{tabular}}}}}

\theoremstyle{definition}
\newtheorem{practice}{Practice}

%-----------------------------------------------------------------------------
%
% Commands for annotating the docs with fixme and inter-author notes.  See
% below for how to disable these.
%
% Define a \fixme command to mark visually things needing fixing in the draft,
% as well as similar commands for each author to leave initialed special
% comments in the document.
% For final printing or to simply disable these bright warnings, copy
% (there's a target macros_off' in the makefile that does this) the file
% macros_off.tex to macros.tex
\usepackage{amssymb}

\newcommand{\fix}[1] { \textcolor{red} {
{\fbox{ {\bf Fix:} \ensuremath{\blacktriangleright }} {\bf #1}
\fbox{\ensuremath{\blacktriangleleft} } } } }

% And similarly, one (less jarring, with fewer symbols and no boldface) command
% for each one of us to leave comments in the main text.
\newcommand{\fperez}[1] { \textcolor{blue} {
\ensuremath{\blacklozenge} {\bf fperez:}  {#1}
\ensuremath{\blacklozenge} } }

\newcommand{\jarrod}[1] { \textcolor{darkgreen} {
\ensuremath{\bigstar} {\bf jarrod:}  {#1}
\ensuremath{\bigstar} } }

\newcommand{\mref}[1] { \textcolor{darkorange} {
\ensuremath{\blacksquare} {\bf missing ref:}  {#1}
\ensuremath{\blacksquare} } }

%% Uncomment these to turn all the special marker commands off
%\renewcommand{\fix}[1]{}
%\renewcommand{\fperez}[1]{}
%\renewcommand{\jarrod}[1]{}
%\renewcommand{\mref}[1]{}

\begin{document}

\title{Dummy title}
\author{Dummy author}
\chapter*{Dummy chapter needed for the \textbackslash chapterauthor command to work later}

\mainmatter

\chapterauthor{K. Jarrod Millman}{Division of Biostatistics\\
School of Public Health\\
University of California, Berkeley}
\chapterauthor{Fernando PÃ©rez}{Henry H. Wheeler Jr. Brain Imaging Center\\
Helen Wills Neuroscience Institute\\
University of California, Berkeley}


\chapter{Two cultures: open source software and scientific research}

\fix{We need to figure out how to move this dedication to the top, below the
  title, where those things typically go}
\emph{Dedicated to the memory of John~D.~Hunter~III, 1968-2012.}
\vspace{5mm}

As members of both the scientific research and the open-source
software development communities, we have observed that the latter
often lives up better than the former to our ideals of scientific
openness and reproducibility.  We explore the reasons behind this,
and argue that these problems are particularly acute in computational
domains where they should be in fact less prevalent.   We discuss
how we can draw specific lessons from the open source community both
in terms of technical approaches and of changing the structure of
incentives, to make progress towards a more solid base for reproducible
computational research.

\section{Introduction}\label{intro}

\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
  Science alone of all the subjects contains within itself the lesson of the
  danger of belief in the infallibility of the greatest teachers in the
  preceding generation\ldots Learn from science that you must doubt the experts.
}%
{\textit{What is Science? (1969)}\\ \textsc{Richard Feynman} }

Scientific research has become pervasively computational. In addition
to experiment and theory, the notions of simulation and data-intensive
discovery have emerged as third and fourth pillars of science \cite{4th-paradigm}.
Today, even theory and experiment are computational, as virtually
all experimental work requires computing (whether in data collection,
pre-processing or analysis) and most theoretical work requires symbolic
and numerical computing to develop and refine models. Scanning the pages
of any major scientific journal, one is hard-pressed to find a publication
in any discipline that does not depend on computing for its findings.

And yet, for all its importance, computing is often treated as an
afterthought both in the training of new scientists and in the conduct
of everyday research. Most working scientists have witnessed how computing
is treated as a task of secondary importance that students and postdocs
learn ``on the go'' with little training to ensure that results
are trustworthy, comprehensible and ultimately a secure foundation
for reproducible outcomes. Software and data are stored with poor
organization, documentation and tests. A patchwork of software tools
is used with limited attention paid to capturing the complex workflows
that emerge, and the evolution of code is often not tracked over time,
making it difficult to understand how a result was obtained. Finally,
many of the software packages used by scientists in research are proprietary
and closed-source, preventing the community from having a complete
understanding of the final scientific results.

\subsection{Growing crisis}

The situation above will be familiar to all scientists and this familiarity
may breed complacency. However, the consequences of this situation are serious
at present and will become disastrous if the situation is not dealt with
directly. 

Much more than a ``third branch'' of science, computing has become so central
to scientific practice that it is almost unimaginable to find a working
scientist that could claim they do not use computers at all.  Experimentalist
from biology to physics and observationalist from astronomy to climate study
are confronted with an avalanche of quantitative data.  All scientists must now
do real computing.  As computers continue to increase in computational power and
storage capacity, the problems only compound.  While better computing practices
will not directly translate to better science, current practices do lead to bad
science.

%Consider, just to name two widely publicized cases, the loss of public
%confidence in the``Climategate'' fiasco \cite{Hef10} or the Duke cancer trials
%scandal, where sloppy computational practices likely led to severe health
%consequences for several patients \cite{Cou10}. 

%The situation is more common than we'd like to believe:
%\begin{itemize}
%\item Begley \& Ellis, Nature, 3/28/12: {\emph Drug development:
%Raise standards for preclinical cancer research.}
%\item 47 out of 53 ``landmark papers'' could not be replicated.
%\end{itemize}
%See Nature, Feb 2012, Ince et al: {\emph The case for open computer programs}
%\begin{itemize}
%\item The scientific community places more faith in computation
%than is justified
%\item anything less than the release of actual source code is an
%indefensible approach for any scientific results that depend on computation
%\end{itemize}
%
%
%The ability to reproduce and verify results is a hallmark of the
%scientific method.
%
%A crisis of credibility and real issues
%
%Retraction rates are going up \cite{cokol2008retraction,steen2011retractions} 
\subsection{Contrast of cultures}

Open source software development uses public fora for most discussion
and systems for sharing code and data that are powerful
provenance tracking systems. There is a strong culture of public disclosure,
tracking and fixing of bugs, and development often includes exhaustive
validation tests that are executed automatically whenever changes
are made to the software and whose output is publicly available on
the internet. This helps with early detection of problems, mitigates
their recurrence, and ensures that the state and quality of the software
is a known quantity under a wide variety of situations (operating
systems, inputs, parameter ranges, etc). Additionally, the same systems
that are used for sharing the code track the authorship of contributions.
All of this ensures that open collaboration does not dilute the merit
or recognition of any individual developer, and allows for a meritocracy
of contributors to develop while enabling highly effective collaboration.

In contrast, the incentives in scientific research are strongly
biased towards the rapid publication of novel results without any serious
requirement of validation. The outcome is that results from publications
in computationally-based research, which means essentially any type of
scientific research, are often impossible to reproduce. Sometimes
this is due to the code not being available at all in the first place.
Authors often do make codes available---thus fulfilling a token requirement
of disclosure---but in such a state that this disclosure is not a
practical solution to the reproducibility problem. A static archive
of source code that has never been tested in a computer or operating
system outside of the author's, never been audited by external eyes
and with no automatic testing built into it, is highly unlikely to
work reliably when used in a completely new environment.

\subsection{Pragmatic approach}

Notwithstanding the above, there are real issues with attempting to
naively transplant the practices of open source development directly
to computational research. The open source model ends up being one
where, in practice, the copyright and authorship of any large collaborative
project is spread among many authors, possibly thousands. While
the source control tools in use do allow for a relatively precise
provenance analysis to be performed if desired, this is rarely done
and its success is contingent on the community having followed certain
steps rigorously to ensure that attribution was correctly recorded
during development.

This is not a major issue in open source, as the rewards mechanisms
tend to be more informal and based on the overall recognition of any
one contributor in the community. Sometimes people contribute to open
source projects as part of their official work responsibilities, and
in that case a company can enact whatever policies it deems necessary;
often contributions are made by volunteers for whom an acknowledgment
in the project's credits is sufficient recognition.

In contrast, the academic world overwhelmingly weighs the authorship
of scholarly articles and conference proceedings as the main driver
of all forms of professional advancement and reward. In this system,
the pecking order of authorship matters enormously (with the many
unpleasant consequences familiar to all of us), and so does the total
number of authors in a publication. While in certain communities papers
with thousands of authors do exist (experimental high-energy physics
being the classic example), most scientists need the prominent visibility
they can achieve in a short author list. The dilution of authorship
results from a largely open collaborative development model
is an important issue that must be addressed.

Furthermore, the notion of a fully open development model typical
of open source projects is at odds with another aspect of the
scientific publication and reward system: the ``first
to publish'' race. Many scientists are, understandably,
leery of exposing their projects on an openly accessible website
when in their embryonic stages. The fear of being scooped by others
is real, and again we must properly address it as we consider
how to apply the lessons of open source development to the scientific
context.



\parasep

In the next section, we share lessons we've learned from our participation in
several open source development projects and describe how these lessons can be
applied to scientific practice.  In the following section, we present a
comprehensive view of the role computing plays in the life-cycle of scientific
investigation.  In addition to surveying the standard approaches, this section
ends with a detailed overview of the open source IPython project
\cite{PER-GRA:2007}, which is a single software tool capable of spanning the
entire life-cycle of computational research.  Finally, we conclude with a brief
discussion of how we see the culture changing and the need for reviewing our
current incentive models and the training of new scientists.

\section{Lessons from open source}\label{lessons-oss}

Over the last decade, we've had the privilege of participating in a loose-knit
community of scientists, researchers, and engineers working to create a powerful
stack of open source tools for scientific computing written in Python.  Among
high-level open source programming languages, Python is today the leading tool
for general-purpose scientific computing (along with R for statistics),
finding wide adoption across research disciplines, education and industry and
being a core infrastructure tool at institutions such as CERN and the Hubble
Space Telescope Science Institute
\cite{millman2011python,Perez2011,ganga09,SST}.


core tools: numpy, scipy, matplotlib, and ipython

- simultaneously we've interacted heavily with researchers in
physics, psychology, neuroscience, and medicine.

- we've also talked with members of SciPy community representing
other disciplines and 


\subsection{Two cultures}

In his influential 1959 Rede Lecture, C. P. Snow lamented the separation between the
two cultures of science and humanities.  In particular, he warned that a lack
of understanding of scientific ideas among the intellectual classes was a major
impediment to solving the problems of modern society \cite{snow1960two}.

\begin{quote}
A good many times I have been present at gatherings of people who, by the
standards of the traditional culture, are thought highly educated and who have
with considerable gusto been expressing their incredulity at the illiteracy of
scientists. Once or twice I have been provoked and have asked the company how
many of them could describe the Second Law of Thermodynamics. The response was
cold: it was also negative. Yet I was asking something which is about the
scientific equivalent of: Have you read a work of Shakespeareâs?
\end{quote}

Similarly we've noticed a separation between the predominant culture of
science today and that of open source software development. Among the
many smart, highly-trained, and well-educated scientists we've been
fortunate to know and work with, it is remarkable how few have basic
fluency with the standard best practices developed for software development.
Even among widely-used scientific analysis software, it is surprising how
jmany are developed without following these practice by routine.

When encouraging our scientific colleagues and friends to adopt, what we
consider, basic best practices for their software development, we've often
encountered a strong resistance and a litany of dismissive rejoinders ranging
from ``We're not software engineers'' to ``We're not curing cancer''.%
\footnote{Comment on Potti cancer fiasco.}

Because we've encountered so much resistance among scientists to following best
practices when writing software, we briefly argue for the necessity for
scientists to learn these lessons so basic to the culture of open source
software development.

%\paragraph{ {\bf Nobody's word}} 
%
%\setlength{\epigraphrule}{0pt}
%\setlength{\epigraphwidth}{.90\textwidth}
%\epigraph%
%{% 
%  \emph{Nullius addictus iurare in verba magistri,
%  quo me cumque rapit tempestas, deferor hospes.}
%
%  I am not bound over to swear allegiance to any master; where the storm
%  drives me I turn in for shelter.
%}%
%{\textit{Book I, epistle i, line 14.}\\ \textsc{Horace} }
%
%While a detailed discussion of the origin of modern science is beyond the scope
%of this chapter, we briefly want to address how the scientific method arose, in
%part, as a rejection of authority as the source of knowledge. Over the last
%several centuries, the idea that knowledge of the natural world must be based
%on observation and experiment has become so pervasive that it seems almost
%self-evident. Academic thought, however, in Europe's medieval universities
%centered on the study of classic texts (particularly from the Greeks). The
%scientific approach---appealing to facts over authority---was widely debated
%and its ascendance was not a foregone conclusion \cite{shapin2011leviathan}.
%
%Ideas such as controlled experiments, repeatability, and hypothesis/prediction,
%slowly coalesced during the 17th century.  Simultaneous with the advance of these
%ideas, scientific communities began arising across Europe to discuss and promote
%them. The Royal Society of London for Improving Natural Knowledge (the Royal Society),
%perhaps most notable of these communities, was founded in November 1660 with the motto
%'Nullius in verba', roughly translated as 'Take nobody's word for it'. This motto
%succinctly captures the ethos of this new approach to knowledge that prized above
%all the ability of individuals to verify for themselves over received wisdom.
%
%As computation pervades the scientific endeavor, it becomes important to assure
%that we do not lose this central aspect of the scientific approach.

%\paragraph{ {\bf Cargo cult science}}
%
%\setlength{\epigraphrule}{0pt}
%\setlength{\epigraphwidth}{.90\textwidth}
%\epigraph%
%{%
%  In summary, the idea is to try to give all of the information to
%  help others to judge the value of your contribution; not just the
%  information that leads to judgment in one particular direction or
%  another.
%}%
%{\textit{Cargo Cult Science (1974)}\\ \textsc{Richard Feynman} }
%
%%popularized term after his 1974 Caltech commencement speech
%
%Anthropologist have documented a curious phenomena arising among several tribal
%island societies after encountering the material wealth of modern society. As
%explorers and traders arrived on remote Pacific islands, during the late
%nineteenth century and early twentieth century, they brought manufactured goods
%with them.  Unfamiliar with modern industrial manufacturing processes, the
%pre-industrial tribal societies often attributed the cargo to divine, rather
%than human, origin. Among these societies, there are several documented
%incidents of ``cargo cults'' arising to bring more cargo to their islands.
%These cults are documented to have attempted to reproduce the rituals associated
%with the arrival of cargo---constructing mock runways lined with fires and manned
%by islanders in fake control towers---in the mistaken
%belief that these rituals could cause the arrival of airplanes laden with
%cargo.
%
%While there is a continuing debate about the meaning and status of cargo cults
%among anthropologist \cite{jebens2004cargo}, the phrase has resonated among both
%scientists and programmers leading to discussions of ``cargo cult science'' and
%``cargo cult programming''. Ritualistic practices mimicking the form of past
%attempts abound. We argue that the scientific publication, which lies at the
%heart of scientific practice, is in danger of becoming such a ritual. We are
%by no means the first to remark on this danger.
%
%In the early 1960s, Peter Medawar pointed out that the classic form of the
%scientific publication was a fiction \cite{medawar1963scientific}. Still
%it is common for editors to insist that accepted publications follow the
%standard Introduction, Methods, Results and Discussion (IMRAD) structure of the
%scientific paper \cite{sollaci2004introduction}.
%
%Almost 300 years earlier, the Royal Society's Philosophical Transactions (1665)
%edited by Henry Oldenburg appeared. As one of the first publications that could be
%recognized as a scientific journal, the Philosophical Transactions provided a
%mechanism to disseminate work and provide attribution. Early journals, such as this,
%initially left acceptance and review of submissions to the discretion of their
%editors. As the number and diversity of submissions increased, new review procedures
%evolved to meet the growing needs of the scientific community. At first, small
%editorial boards were formed to assist editors in soliciting, reviewing,
%and recommending articles worth of publication. Existing technologies greatly
%limited journals in this process. Duplication and distribution of manuscripts
%to be reviewed was greatly aided by the arrival of new technologies such as
%the typewriter, carbon copies, photocopies, and eventually the arrival of
%personal computers and the internet \cite{spier2002history}.
%
%``
%    new technologies are again changing scientific publications
%        online publications: preprints, continuous revision, open discussion
%    new technologies are also changing the everyday practice of science
%        increased data storage is rapidly expanding the amount of experimental data we can acquire and analyze
%        increased computational power is vastly increasing our ability to model and
%''



\paragraph{ {\bf Computational practice}}

\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
  In ordinary computational practice by hand or by desk machines, it
  is the custom to check every step of the computation and, when an
  error is found, to localize it by a backward process starting from
  the first point where the error is noted.
}%
{\textit{Cybernetics (1948)}\\ \textsc{Norbert Wiener} }

Computing is now synonymous with the use of automated, electronic machines.  So
synonymous, in fact, that is easy to forget that computing has always been part
of scientific investigation. Of course, the types of problems amenable to
computing by hand on paper or with early calculating machines (e.g., abacus)
were limited. 

Lessons we all were taught in our elementary mathematics classes:

- show your work

- need to double check answers

- ease of making a mistake

As automated, electronic computing machines were developed by scientists,
those same grade school practices were standard.  Code and methods were
openly shared and discussed, rigorous testing procedures, etc.

The early days of computing \emph{were open}: von Neumann's reports
\cite{grcar2011john}.

\paragraph{ {\bf Software crisis}}

\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
  The major cause of the software crisis is that the machines have become
  several orders of magnitude more powerful! To put it quite bluntly: as long
  as there were no machines, programming was no problem at all; when we had a
  few weak computers, programming became a mild problem, and now we have
  gigantic computers, programming has become an equally gigantic problem.
}%
{\textit{The Humble Programmer (1972)}\\ \textsc{Edsger Dijkstra} }

By the early 1970s, computer scientists, software engineers, and project
managers were facing a growing ``software crisis''. The explosive growth in
computing power had quickly out-paced the ability of humans to tell those machines
what to do. Major software projects were routinely running over budget and
time. Software bugs were causing huge monetary losses and even death as code
was beginning to control everything from medical devices to industrial processes.

No silver bullets \cite{brooks1995mythical}.

%\paragraph{ {\bf Open source}}
%
%\setlength{\epigraphrule}{0pt}
%\setlength{\epigraphwidth}{.90\textwidth}
%\epigraph%
%{%
%  a great babbling bazaar of differing agendas and approaches \ldots
%}%
%{\textit{The Cathedral and the Bazaar (2000)}\\ \textsc{Eric Steven Raymond} }
%
%Computing started to close up in the 70's, AT\&T's lock-down of Unix
%and Bill Gates' letter to computer hobbyists (Jan'76). Stallman's
%story with printer drivers, a reaction to centralized, locked-down
%models: the GNU/FSF movement is born. Linux: OSS for the masses. The
%rise of the internet as powered by Linux.


\paragraph{ {\bf Scientist programmer}}

Early computers were extremely costly and only available to highly mathematical
engineers and scientists. As continued improvements in manufacturing processes
drove down costs and increased the speed and storage capacity of computers, their
prevalence in scientific practice has rapidly grown.

Scientists as developers

Reasons

\begin{itemize}
\item  developing state-of-the-art methods
\item developing as exploration
\end{itemize}

Implications

\begin{itemize}
\item need development tools to enable
\item scientists will need to be computationally literate
\end{itemize}

Purpose:  Better Research

\begin{itemize}
\item Science becomes computing
\item Both building a hierarchical structure
\item freedom to believe differently
\end{itemize}




\subsection{Best practices}

\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
  when a man tries all kinds of experiments without method or
  order, this is mere groping in the dark; but when he proceeds with
  some direction and order in his experiments, it is as if he were
  led by the hand
}%
{\textit{Novum Organum (1620)}\\ \textsc{Francis Bacon} }

The practices recommended in this section are distilled from years of
experience writing and maintaining software, teaching programming courses to
students and scientists, as well as extensive interaction and discussion with a
diverse group of scientists and engineers.  Whole books have been dedicated to
best practices in software development with highly specialized tools and habits
for individual programming languages and tools.  In this short section, we
highlight the practices and tools essential to any computational work.

We begin by discussing practices and tools that should be applied to even
exploratory, individual research.  These practices are so essential to
efficient and productive use of computational resources that we routinely use
them whenever we use a computer. In the following section, we discuss how these
practices and tools naturally extend to collaborative work. 

\paragraph{ {\bf Version control}}

Whether you are collecting data, running analyses, or writing papers,
you will inevitably need to keep track of the various versions of your work:
data is augmented and curated; code is adapted and improved; and writing is
revised and expanded.  While simply keeping the most recent version of your
work sounds like a reasonable approach, in practice, this is seldom sufficient.
There are tentative new directions, detours, and dead ends.

We've witnessed numerous researchers attempting to manage different versions of
their work using manual and laborious kludges. The most common patterns include
using \emph{ad hoc} naming schemes (e.g., \texttt{file.txt.bak},
\texttt{file.txt.1st}, etc.), emailing different versions to yourself, or using
the specialized functionality built into the tool you happen to use (e.g.,
Microsoft Word's ``Track Changes'' functionality).  While these approaches are
partial solutions to the problem, they are also cumbersome and prone to
failure.  More importantly, they are completely unsustainable beyond very
simple scenarios with only one or two files and do not scale to any kind of
sensible collaboration workflow.

Because tracking and managing how work evolves over time is so fundamental to
the workflow of software development, programmers have created specialized
software tools to do exactly this. These tools are called \emph{version
  control} systems. Several open source version control (VC) systems have been
developed over the years, the most well known being CVS, SVN, Git, and
Mercurial.

While there are notable differences among these tools, they all share some
basic concepts.  In most VC systems, you store all your project files (code,
text, figures, etc.) in a \emph{repository} (often represented on disk in a
directory hierarchy).  There are commands to add and remove files to a
repository.  When you change a file in your repository that you want to track,
you can \emph{commit} those changes with a \emph{commit message}.  The
repository and commit mechanism provide a complete historical log of the
project from inception to current state, including every change made along with
timestamps, author, comments and other metadata for each modification to the
codebase.

Commonly your changes may follow a simple linear progression of commits.
However it is reasonable to have a project evolve so that it does not follow a
straight route from beginning to end. For example, given the exploratory nature
of research, you may have several alternative approaches that you want to try
out. In such cases, your commits will resemble a tree with several
\emph{branches} diverging from a common base or trunk. When exploring these
alternative approaches on different branches, you may find that several
branches converge. At this point, you will need a way to \emph{merge} these
branches back together.  If the changes in each of these branches don't overlap
with one another, the VC system is capable of merging them all together in a
completely automated fashion.  When you have \emph{conflicting} changes in
different branches (e.g., edits to the same line of code), then the VC systems
require manual intervention where you have to choose which of the conflicting
changes should ultimately prevail.  But in all cases, a VC system is the only
reasonable solution for managing the evolution of multiple branches of parallel
development in a set of files (whether written documents, computer code or
data).

In the design of more modern VC systems such as Mercurial and Git, an important
consideration has been woven into the core of the system: built-in \emph{data
  integrity verification} via cryptographically robust fingerprinting of all
content.  The basic idea is that at every commit, the VC system computes a
``fingerprint'' of the content being committed as well as the data it depended
on\footnote{More precisely, a hash function is evaluated on the content of the
  commit and the hash of all commits it depends on, which creates a directed
  acyclic graph of hash values that signs the entire repository.  Today these
  systems employ the SHA1 hash function, but other hashes could be equally used
  if necessary}.  In this manner, at any point in time it is possible to
establish the integrity of the entire history of a repository, by recomputing
these fingerprints and comparing them against what has been stored.  By the
nature of hash functions, if even a single byte is changed in the entire
repository all subsequent hashes will be modified and the change can thus be
detected.  This key design idea is used by VC systems for all kinds of internal
operations, but it also means that when a scientist gets a copy of a
repository, he or she can be confident, with near mathematical certainty, that
the entire history contained therein has not been tampered with in any way when
copmared to another repository with the same hashes.  

Strong guarantees on data integrity are a necessary condition of any
reproducible workflow, and one of the reasons why we emphasize so much the
pervasive use of modern verstion control systems as the foundation of a
reproducible research environment.

It is important to note that VC systems were developed originally for the
management of human-generated content such as computer source code or text
documents, not for the handling of large binary data that is common in science.
By virtue of their design, they tend to be somewhat inefficient if one attempts
to store all the changes in a project with many frequently changing large
binary files, and this somewhat limits their use for the tracking of all assets
in a research project.  But new efforts exist to mitigate these limitations,
such as the git-annex project\footnote{\url{http://git-annex.branchable.com}}
that uses git itself for storing all metadata about large binary assets, along
with a static (configurable) storage resource external to git for the assets
themselves.  This approach makes it possible to smoothly integrate the
management of binary data within a VC workflow, without creating an explosion
in the size of the VC storage area.

In summary, the use of version control should be seen as a ''background
technology''; in our case its use is so routine that we use it for everything:
writing grants and papers, teaching materials, homework, slides for talks, etc.
This leads us to suggest our first ``recipe'' in terms of best practices:

\begin{practice}
  Pervasive version control: research codes, manuscripts and data analysis
  projects should be developed, from the beginning, \emph{always} using version
  control systems that track the actual history of everyone's contributions.
\end{practice}

\paragraph{ {\bf Testing}} Computing is error-prone. While there is no
fool-proof way to rid computing of error, there are ways to limit and reduce
it. One of the most successful and widely used techniques involves
comprehensive testing, so that bugs (i.e., errors) are found quickly. Finding
bugs as soon as possible in the development process is extremely valuable.
Depending on the nature of the bug, it may reveal a fundamental problem with
the overall design of your code requiring months more of coding.  Even small
errors that are easily fixed may require rerunning months of analysis.
Debugging is the process of fixing bugs that you find and it can be a
painstaking ordeal. To reduce the amount of time it takes to uncover bugs and
to ease the pain of debugging your code, it is essential to adopt a rigorous
testing practice up front.%
\footnote{While testing is extremely useful
practice, we should also point out that it is often much more interesting work
than debugging.} Beware though, as E. W. Dijkstra famously observed, ``Program
testing can be used to show the presence of bugs, but never to show their
absence!'' \cite{dahl1972structured}

Testing must be performed on multiple levels. If your program takes input
either from a user or file, then it is important that the code checks that the
input is exactly what the program expects. You will also want to write
\emph{unit tests} to ensure that unitary code elements like functions, classes,
and class methods behave correctly. And finally, you will want to write
\emph{regression tests} to make sure you do not break existing functionality.
For instance, you could have a previous analysis rerun to make sure that the
program reproduces your previous results.

Testing should be done as early as possible in the development process.
Some programmers go so far as advocate Test-Driven Development (TDD), an
iterative programming methodology consisting of repeatedly writing
unit test cases and then implementing only the code necessary to pass them
\cite{Bec02, Ast03}.  TDD is a continuous cycle of three steps:
\begin{enumerate}

\item {\bf Fail Test.} Before implementing any new functionality, first
write a failing test case (it should fail because the new functionality doesn't
exist yet). This forces you to think about what you want a
piece of code to do, rather than how it does it.  It also validates that the
unit test fails and that new functioning code is required for the test
to pass. By starting the development of each new feature with the creation of a
unit test, you focus on the code interface rather than its implementation, and
you also have a simple examples of code use that may be helpful later.

\item {\bf Pass Test.} Do not worry whether the code is elegant or
optimized. The only aim is to produce code that passes the test and introduces
no functionality, which does not already have a unit test.

\item {\bf Refactor.} Once your code passes the unit test, you can then iteratively
refine the code to make it easier to read and understand as well as removing code
duplication. As your code evolves, these tests let you know that it still
behaves correctly.

\end{enumerate}

This practice ensures that all written code is tested and limits the number of
defects. It improves code quality by forcing you to focus on use cases rather
than getting lost in implementation details. By thinking about the test from
the outset, you can avoid finding that the code you just wrote is a huge
untestable mess. It can also improve documentation because an example (i.e.,
the test case) is often better than an explanation. And if you regularly run
the test, you will quickly know when your code no longer works for the example
(something you may never notice in the case of explanatory text).  Finally, you
will have more robust code as you will more quickly isolate bugs, which makes
them easier to fix.\cite{oram2010making}


\paragraph{ {\bf Readability}}
\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
This emphasis on readability is no accident. As an object-oriented language,
Python aims to encourage the creation of reusable code. Even if we all wrote
perfect documentation all of the time, code can hardly be considered reusable
if it's not readable.
}%
{foreword to \textit{Programming Python (1st ed.)}\\ \textsc{Guido van Rossum} }

While writing code that is well-tested and systematically managed by a modern
VC system is important, code that you (and your colleagues) can't read is
nearly useless. Readable code is written with explanatory names, clear logical
structure, and comprehensive documentation where necessary.  There is an
extensive and growing literature (e.g., \cite{boswell2011art, Fow00,
kernighan1999practice, HT00, mcconnell2009code}) on ``stylistic'' aspects of
good programming. Because scientific papers and grant submissions have become
the currency of the scientific realm, it is probable that a good number of our
colleagues have spent at least some time reading classics such as Strunk \&
White's \emph{Elements of Style}. Yet, even as an increasing amount of their
work is produced in lines of code, there is a paucity of scientists paying the
same attention to the elements of good programming style. The emphasis on
readability is included in this section because even when you are the only one
using or working on your code, the chance that you will need to read your own
code is high. Even when your code is widely used and shared, the chances are
that you still will be the one most frequently reading your own code.

Given the literature on the subject, written by more experienced programmers,
we will only briefly touch on self-documenting code. Self-documenting code, as
the name implies, reduces the need for external documentation by placing an
emphasis on clear, well-written code that is easy to read and understand.  In
mathematics, it is accepted practice to follow established naming conventions
(e.g., capital letters for sets and lower case letter for elements). It is
equally expected that when making a mathematical argument, one shouldn't
arbitrarily switch from functional to relational notation.  Similarly, using
consistent and uniform naming conventions when programming should be standard
practice. Brevity in naming should be balanced against explicit and descriptive
words. For example, you might use the term \texttt{download} rather than
\texttt{get} in a function call to download a specific dataset from the
internet. Expressions are the next block to readability. While mathematical
manipulation (e.g., De Morgan's laws) can be used to great effect in making
your expressions more easily understood, it is often important to use the right
level of abstraction. Higher level programming languages (e.g., Matlab, Python,
and R) provide data structures (such as n-dimensional vectors or statistical
formulas) that enable the code to be understood at the level of the
mathematical ideas. Finally, the overall control flow of your code must be
clear and easy to follow. Finding the best control flow requires a deep
understanding of your problem and an in-depth knowledge of programming
methodology and the specifics of the language you are working in. Like good
writing, good coding is achieved through deliberate practice.

Readability in programming encompasses not just the code itself,
but the accompanying documentation. Code comments offer much to readability,
but are sadly used to little positive effect. Inspired by the idea of
self-documenting code, we've heard programmers argue that good code doesn't
need comments. Indeed, liberally commenting your program to counter poorly
written, obscure code is poor practice. Comments that simply explain how a piece
of code works are often unhelpful. If code is so obscure to need explanation,
it is perhaps better to rewrite or refactor it. One problem with comments (as
with many types of documentation) is that it is often uncoupled with the actual
code. This means that there is no way to ensure that the two don't diverge.
And, if they diverge, it may not be obvious which is at fault.

To illustrate how comments and documentation can enhance readability of your
code, we discuss the commenting and documentation system that has been
developed by NumPy and is being used by other scientific Python projects. While
this section is specific to the tools and processes put in place in the
scientific Python community, the general ideas are more broadly applicable.

By 2007, NumPy provided the foundation layer for numerical, scientific
computing in Python. With over a decade of development effort behind it, NumPy
had evolved (with the help of numerous developers and users) to a powerful and
widely used package. However, there was a lack of good reference information
for the various functions, classes, and modules it provided. Users and
developers had access to the source code, an almost 400 page ``Guide to
NumPy'', and an active mailing list. Yet, it was clear that this level of
documentation wasn't enough. To address this, the community began a year long
effort to develop a documentation string standard. In Python, a documentation
string (or docstring) is any string in the first line in an object's (function,
class, etc.) definition. Since docstrings are embedded in the source code, they
are ready available to anyone directly viewing the source. The string is
associated with the object and can be programmatically accessed as well.  They
can be accessed from an interpreter using any object's \texttt{\_\_doc\_\_}
attribute.  Docstrings can also be accessed for autogenerating documentation.
Similar functionality exists in other programming languages such as R. And even
in language that don't include this functionality, it is common practice to
include comments at the beginning of object definitions that are used
similarly.

Given a recognized need for better documentation and documentation
functionality that we wanted to leverage, the discussion focused on what this
documentation should include. Besides the need to include information such as a
brief statement of purpose as well as input and output parameters, there were
several issues that arose with special relevance for scientific applications.
For instance, many of the algorithms had simple mathematical expressions that
weren't necessarily obvious from there implementation in Python. In these
cases, a few equations written in \LaTeX{} were immediately useful. Moreover,
in several cases we were implementing functionality that was described in
peer-reviewed academic journals. And, finally, we could provide extremely
short examples of how to use the code. As an added benefit, Python makes it
easy to include examples in the docstrings as part of the test suite; this
improves the test coverage and helps ensure that the documentation doesn't get
out of sync with the actual code.

\paragraph{ {\bf Infrastructure}}

For very small projects, managing everything by hand may be straightforward.
But as your research project (code, data, and text) evolves, the burden of
running your tests, building your project, and generating reports will become
overwhelming. Eventually you will need tools and procedures in place to take
care of these details for you. Even when the project is small enough that you
can manually manage things, automating these tasks can be extremely beneficial.
\cite{doar2005practical}

\begin{itemize}

\item Tests runners and continuous integration, coverage

\item Make vs IDE

\item auto-generated API documentation, NumPy documentation system \cite{SciPyProceedings_27}

\item online resources: trac, redmine, vs hosted project management like
      sourceforge, github

\end{itemize}

%documentation, code, figures/results, process, distribution, etc.
%
%auto-generated API documentation, bug tracker, task list, project management
%
%
%Sphinx
%
%* Official documentation tool of python
%
%* And numpy, scipy, ipython, matplotlib, mayavi, \ldots
%
%* Content is searchable, indexed, and cross-referenced
%
%* Generates HTML and PDF
%
%* Extensible!
%
%A good documentation tool should
%
%* Be easy to use
%
%* Readable in plain text format, *but* produce beautiful HTML and PDF
%
%* Let you use the editors and tools you prefer
%
%* Facilitate collaboration


\subsection{Collaboration}

Collaboration is essential to both the scientific and open source
communities. Open source developers build on one another's work just
as scientists build on each others work. As scientific practice
becomes increasingly computational, it is imperative that we
learn from the collaborative practices used in the open
source world.

We've noticed several ways that the current lack of collaboration
among scientists on software tools is impeding progress.

\begin{itemize}
\item lab-based software package competition
\item state-of-the-art algorithms seldom used (code not available, code not usable)
\item solved problems in theory---not in practice
\item competition at the level of software---not algorithms
\end{itemize}


The practices and tools in open source software development are much more
advanced when it comes to collaboration. Since development communities are
geographically spread and often dependent on contributions from volunteers,
there has been careful attention paid to efficient and productive tools and
processes for managing collaborations.


\paragraph{ {\bf Distributed version control}}

Earlier we discussed how version control (VC) should be the foundation of a
reproducible research workflow, even for a single investigator working in
isolation.  But the true power of these systems comes into its own when
considering the need to collaborate with others.  Modern systems such as Git
and Mercurial were designed from the ground up for large-scale distributed
collaboration: Git was written by Linus Torvalds, the creator of the Linux
kernel, to coordinate its development.  The Linux kernel is arguably the
largest and most complex single open source development project today: version
3.7 of the kernel included roughly 12,000 distinct sets of changes affecting
over 1,100,000 lines of code by nearly 1,300 individual
contributors\footnote{url{https://lwn.net/Articles/526748}}.  Git's entire
design aims to make collaboration on this scale smooth and efficient, and it
succeeds admirably at the task.  Scientists can benefit from this power as well
for any project that requires collaboration, whether it is the development of
an open source research code or the writing of a manuscript or grant proposal
with multiple authors.

Git and other tools like it are called \emph{distributed} version control
systems (DVCS) because they don't on any central server for their functioning,
instead maintaining an entire history of a project inside every repository.
This is in contrast to legacy systems such as CVS and SVN, that made a
distinction between the ``working copy'' that users would work on and which
contained only the most recent version of files, and a special repository
hosted on a central server that had the entire project history.  The
centralized model does enable collaboration, but it also creates a number of
problems that the distributed model addresses.  In a DVCS there is no single
point of failure, as every repository carries all the project history and
therefore serves as an automatic backup.

More importantly, a DVCS enables anyone who can \emph{clone} a repository (the
term used to indicate getting a full copy of an existing repository to start
new work off of it) to develop their own history with new commits, even if they
don't have write permissions to the original source that they cloned from.
This means that once you have a clone a repository (which could be someone
else's or yours from a different computer), you can start working on that copy
and building new history even if you are disconnected from the original system,
such as when working on a plane or train without internet access.  If at a
later stage you decide to merge your new history with the original repository,
the merge capabilities in all DVC systems make this very straightforward.

This model of cloning an existing repository, building new history in isolation
and then merging it back into a common history, is the basis for how these
systems enable a very fluid workflow for collaboration.  When the time comes to
need a merge operation, DVC systems can communicate the necessary changes even
via email attachments, but the simplest way to do so is to have a special
repository\footnote{We note that this central repository does not change the
  distributed nature of the process: while it plays a special role for purposes
  of \emph{synchronization}, the central repository is otherwise completely
  symmetrical to everyone's personal copy in the information it holds, and can
  be replaced at any time in case it is lost or damaged from anyone's copy.} in
a common location that all parties have access to and where the changes are
\emph{pushed} to.  Pushing, as the term suggests, means sending the set of
changes from one repository into another; once the changes have been put into
this central repository, all parties can \emph{pull} them into their personal
copies to synchronize their states and continue working again.  So in practice,
the simplest and most common collaboration workflow with a DVCS is one where
each person has a copy they develop on, and they all connect in a star topology
to a central node where a shared copy exists that is used for synchronization.

In recent years, a number of web services have appeared that play the role of
this central node; the most popular of them by far is
GitHub\footnote{\url{http://github.com}}, but others such as
BitBucket\footnote{\url{https://bitbucket.org}} and
Gitorious\footnote{\url{http://gitorious.org}} play similar roles.  GitHub has
had a tremendous impact in the open source community, reaching in a few years
millions of active users and gaining rapidly popularity in scientific circles.
We can attest to the power of this platform with our own experience: the open
source IPython project (see Â§~\ref{IPython}) moved its development to GitHub in
early 2010 and immediately saw a rapid uptick in the pace of contributions.
The workflow for collaboration enabled by GitHub was so much smoother than all
previously availalbe tools that many people were more willing to send
contributions, while the core team was able to review and integrate these
contributions at a much more rapid pace.

The core element of the collaborative process on GitHub is known as a
\emph{pull request}, and it is something akin to a public peer review of a set
of changes to a manuscript.  Let us illustrate how it works with a simple
example: Alice wants to contribute to IPython, a project available on github at
\url{http://github.com/ipython} but to which she does not have write access.
She can do so by getting her own personal copy of the IPython git repository
where she makes all the changes she wants, and once she is ready to share them
with the IPython team she can publish them on her GitHub user account at
\url{http://gitub.com/alice/ipython} (presuming her GitHub user name is
\texttt{alice} for the sake of this example).  At that point, she can click on
a button to create a ``pull request'' for these changes: this contacts the
IPython developers and creates a special page on the website that summarizes
her changes as well as allowing everyone to begin a discussion about the nature
of these changes.  This discussion page allows the developers to ask Alice
questions (even making comments on specific lines of her new code), and she can
respond to these questions, update her code with new commits in order to
address any required improvements, etc.  Once the IPython developers are
satisfied with this review and discussion (which may happen immediately or may
require a lengthy back-and-forth process, depending on the nature of the
changes), they can apply the changes to the official IPython repository with a
click of a button\footnote{This is an example of a real pull request page for
  IPython that already went through the entire review process:
  \url{https://github.com/ipython/ipython/pull/1732}}.  Once the changes are
merged, they become part of the official project source and every individual
commit that was merged is credited to Alice from the time she made it while she
was working on her personal copy.  Furthermore, even closed pull requests
remain available on the website to inform future discussions, making the entire
collaboration process an open one.

This pull request process allows for a dynamic and open peer review process of
all proposed changes to a project, while everyone has access to the same
version control tools while they work.  The only special role that the official
project authors have is the ability to approve the final merging of new
changes, but otherwise everyone participates on an equal footing in terms of
access to tools.  

In our experience, this highly symmetrical structure proves to be extremely
beneficial in encouraging a very meritocratic process of contribution and
review, where there are very few points of special authority and where the
discussions can remain focused around the contribution that initiated the pull
request.  Paraphrasing how some of the GitHub employees describe the process in
public presentations: ``a pull request is a conversation that starts with
code''.  From a scientific research perspective, we should consider these ideas
in a broader context that goes beyond code: while peer review is one of the
very pillars of how the scientific community moves forward, in practice modern
scientific peer review is often an opaque, arbitrary and limited process.  The
open, dynamic and ongoing process of peer review enabled by the GitHub pull
request system (or the equivalent ones that exist on other similar siervices)
stands in sharp contrast to some of our scientific traditions, and in our
opinion our community could benefit significantly from adopting some of these
ideas in a more pervasive manner in our own review practices
\cite{10.3389/fncom.2012.00018}.

It is worth noting that by using a DVCS, authors can maintain private work
branches in the context of a publicly available project; this can be useful if
new work needs to be developed in practice prior to publication and subsequent
public release. By tracking the public repository but keeping a private branch,
they can maintain exclusive access to their new work until it is published,
while continuing to develop the openly accessible code with the rest of the
scientific community. Once the code is published, since it was developed using
the same version control machinery of the public branch, the new contributions
can be seamlessly merged with the public version and their entire provenance
(including information such as time of invention and individual credit within
the lab) becomes available for inspection.  This simple observation shows how
these tools can be used to balance the sometimes valid requests for privacy
that may exist in a research environment with the desire for subsequent
disclosure and publication, without losing any of the benefits of version
control with regards to attribution and provenance tracking.

\paragraph{ {\bf Infrastructure redux}}

In summary, we think that a few simple lessons can be learned from
the practices of the open source world which, if carefully assimilated,
can lead to substantial improvements in the state of reproducible
computational research. 

For more information, we highly recommend the recent archiv preprint
\ldots \cite{2012arXiv1210.0530A}.

\section{\label{sec:life-cycle}Computational research life-cycle}

A central issue when considering the problem of reproducible research is the
nature and quality of the software tools available for computational work in
science.  In this section, we will present a perspective on the problem of
computational research that takes a unified view of the life-cycle of research
ideas, to show how the majority of existing tools make it difficult to manage
this life-cycle in a way that naturally leads to reproducible outcomes.  We
will briefly review some of the existing prior work dating back to D. Knuth on
literate programming \cite{Knuth92} and mention current progress in that
direction mostly coming from the vibrant R community, and we will then focus on
work recently done in the scientific Python community that takes a slightly
different angle on these questions than the traditional literate programming
approach.

To frame the question, we view the life-cycle of computational research from an
admittedly simplified perspective that is still realistic enough to be
useful. Let us consider that this cycle can be be broken down into the
following phases:

\begin{itemize}
\item \textbf{Individual exploration:} a single investigator tests an idea,
  algorithm or question, likely with a small-scale test data set or simulation.
\item \textbf{Collaboration:} if the initial exploration appears promising,
  more often than not some kind of collaborative effort ensues to bring
  together complementary expertise from colleagues.
\item \textbf{Production-scale execution:} large data sets and complex
  simulations often require the use of clusters, supercomputers or cloud
  resources in parallel.
\item \textbf{Publication:} whether as a paper or an internal report for
  discussion with colleagues, results need to be presented to others in a
  coherent form.
\item \textbf{Education:} ultimately, research results become part of the
  corpus of a discipline that is shared with students and colleagues, thus
  seeding the next iteration in the cycle of research.
\end{itemize}

\subsection{Patchwork of tools}

Let us look at the typical tools and approaches that most working scientists
use for each of these phases today, in particular considering their impact on
the reproducibility of the final outcomes. 

For \textbf{individual exploratory work}, researchers use various interactive
computing environments: Microsoft Excel, Matlab, Mathematica\textregistered\,
Sage \cite{sage}, and more specialized systems like R, SPSS and STATA for
statistics. These environments combine interactive, high-level programming
languages with a rich set of numerical and visualization libraries. The impact
of these environments cannot be overstated; they are used almost universally by
researchers for rapid prototyping, interactive exploration and data analysis
and visualization. However, these environments have a number of limitations:
(a) some of them are proprietary and/or expensive (Excel, Matlab, Mathematica),
(b) most (except for Sage) are focused on coding in a single, relatively slow,
programming language and (c) most (except for Sage and Mathematica) do not have
a document format that is rich, i.e., that can include text, equations, images
and video in addition to source code. While the use of proprietary tools is not
a problem \emph{per se} and may be a good solution in industry, it is a barrier
to scientific collaboration and to the construction of a common scientific
heritage where anyone can validate the work of others and build upon it.
Scientists can not share work unless all colleagues can purchase the same
package and students are forced to work with black boxes they are legally
prevented from inspecting (spectacularly defeating the very essence of
scientific inquiry). Furthermore, because of their limitations in performance
and handling large, complex code bases, these tools are mostly used for
prototyping: researchers eventually have to switch tools for building
production systems.

For \textbf{collaboration}, researchers tend to use a mix of email, version
control systems and shared network folders (Dropbox, etc.).  Version control
systems (Git, SVN, CVS, etc.) are critically important in making research
collaborative and reproducible. They allow groups to work collaboratively on
documents and track how they evolve over time. Ideally, all aspects of
computational research would be hosted on publicly available version control
repositories, such GitHub or Google Code. Unfortunately, an all-too common
approach is still for researchers to email documents to each other with ad-hoc
naming conventions that effectively provide a poor man's version control (and
are the source of endless confusion and frequent mistakes). This form of
collaboration makes it nearly impossible to track the development of a large
project and establish reproducible and testable workflows.  While a small group
can make this work by brute effort, this approach most certainly does not scale
beyond a few collaborators, as painfully experienced by anyone who has
participated in the madness of a flurry of email attachments with oddly-named
files such as {\tt paper-final-v2-REALLY-FINAL-john-oct9.doc}.

For \textbf{production-scale execution}, researchers typically turn away from
the convenience of interactive computing environments to compiled code
(C/C++/Fortran) and parallel computing libraries (MPI, Hadoop), as most
interactive systems do not provide the performance necessary for large-scale
work and have limited support for distributed and parallel computing.  These
tools are specialized enough that their mastery requires a substantial
investment of time. We emphasize that before production-scale computations
begin, the researchers already have working prototype in an interactive
computing environment. Therefore, turning to new parallel tools means starting
over and maintaining at least two versions of the code moving
forward. Furthermore, data produced by the compiled version is often imported
back into the interactive environment for visualization and analysis. The
resulting back-and-forth, complex workflow is nearly impossible to capture and
put into version control systems, again making the computational research
difficult to reproduce.  Obviously the alternative, taken by many, is simply to
run the slow serial codes for a long time and wait for the results while
working on something else.  This is hardly a solution to our questions, as
runtimes in the weeks or months become in practice single-shot efforts that
nobody will attempt to replicate.

For \textbf{publications} and \textbf{education}, researchers use tools such as
\LaTeX, Google Docs or Microsoft Word/PowerPoint.  The most important attribute
of these tools in this context is that, \LaTeX{} excepted, they integrate
poorly with version control systems and are ill-suited for workflow automation.
Digital artifacts (code, data and visualizations) are often manually pasted
into these documents, which easily leads to a divergence between the
computational outcomes and the published version.  Managing this requires
manual updating, something that is error-prone and easy to forget.

From this perspective, we now draw a few lessons:

\begin{enumerate}[(a)]

\item The common approach and tools used today introduces sharp gaps between
  the stages of this workflow.  This forces researchers to switch tools at each
  stage, which in turn makes it difficult to move fluidly back and forth.
  Driven by the pressure to publish, people will then naturally charge
  \emph{forward}, pressing on to assemble results in the chase for an accepted
  manuscript but rarely going back to question assumptions, replicate earlier
  experiments with updated versions of code or parameter tweaks, etc.  Because
  reproducing results effectively requires going \emph{back} to the
  beginning of the pipeline, a workflow that from the onset makes this
  inherently difficult will likely result in outcomes that nobody, not even the
  original authors, can reliably reproduce.

  The pressure to publish in today's world encourages all scientists to charge
  forward chasing the goal of an accepted manuscript, but the very term
  ``reproducibility'' implies repetition and thus a requirement to also move
  \emph{back} to retrace one's steps, question or change assumptions and move
  forward again.

\item A key element of the problem is the disconnect that exists between what
  we view as ``final outcomes'' of the scientific effort (papers and
  presentations that contain artifacts such as figures, tables and other
  outcomes of the computation) and the pipeline that feeds these outcomes.
  Because most workflows involve a manual transfer of information (often with
  unrecorded manual changes along the way), the chances that these final
  outcomes do not match what the computational pipeline actually produces at any
  given time are high.

\item The problems listed above are \emph{both} technical and social.  While we
  focus somewhat on the tools aspect, it is critical to understand that at the
  end of the day, only when researchers make a conscious decision to adopt
  improved work habits will we see substantial improvements on this problem.
  Obviously higher-quality tools will make it easier and more appealing to
  adopt such changes, but other factors are also at play, from the natural
  inertia of ingrained habits to the external pressures applied by the
  incentive models of modern research.
\end{enumerate}

In summary, if we want to ensure that any computational research is truly
reproducible, we need to consider this entire life-cycle in an integral way and
from the beginning of a research project.  Asking about reproducibility by the
time a manuscript is ready for submission to a journal is simply too late: this
problem must be tackled from the start, not as an afterthought tacked-on at
publication time.  We must therefore look for approaches that allow researchers
to fluidly move back and forth between the above stages and that integrate
naturally into their everyday practices of research, collaboration and
publishing, so that we can make simultaneous progress on the technical and
social aspects of this issue.

With the above in mind, our approach to the problem of reproducibility in
computational research will then be focused on building tools and practices
that enable researchers to naturally consider the entire cycle of research as a
continuum, and where ``doing the right thing'' is the easy and natural path
rather than the awkward and cumbersome one.  We will first briefly describe the
existing tools for literate programming as the backdrop against which we will
introduce a contrasting approach that we refer to as ``literate computing''.
We argue that this approach is a better fit to the needs of reproducibility in
computational research and will present tools that have been developed recently
in the open source community in this direction.

\subsection{Literate programming}


\setlength{\epigraphrule}{0pt}
\setlength{\epigraphwidth}{.90\textwidth}
\epigraph%
{%
Instead of imagining that our main task is to instruct a \emph{computer}
what to do, let us concentrate rather on explaining to \emph{human beings}
what we want a computer to do.
}%
{\textit{Literate programming (1984)}\\ \textsc{Donald Knuth} }

The concept of \emph{Literate Programming} was introduced by Donald Knuth in
the early 1980's \cite{Knuth:1983:LP} and a complete description of this
approach to computer programming can be found in his later book of the same
title \cite{Knuth92}.  Knuth's concern was the development of a better approach
to documenting computer software; he devised a process whereby programmers
would write literate source files that describe in full prose the ideas
underlying a given program, interspersed with the code fragments implementing
the actual computations.  Knuth developed tools that can process these input
files to produce two different representations: a ``tangled'' code file meant
for compilation and execution by a computer, and a ``woven'' file containing
the formatted documentation.  Knuth's original implementation, the WEB system
\cite{Knuth:1983:WSS}, was focused on producing Pascal code and \LaTeX
documentation, but this basic idea has been extended to many other programming
languages and documentation systems.  

The R community has actively embraced the ideas behind literate programming,
and a mature implementation of the concept exists for R in the Sweave system
\cite{lmucs-papers:Leisch:2002}.  Sweave is one of the central elements of the
Bioconductor system \cite{Gentleman2004, Dudoit2003} for computational biology
and bioinformatics.  As all Bioconductor packages must be accompanied by at
least one ``vignette'', a literate programming document that contains
executable code illustrating the tasks the package is meant to perform.
Vignettes can be read in PDF format, but functions exist to automatically
extract all the R code for immediate execution.

In recent years, a new entrant to the R community that is gaining rapid
adoption is the Knitr package \cite{xie2012knitr}.  Knitr can be seen as a
highly evolved Sweave with a number of improvements on many fronts, but still
within the conceptual lineage of literate programming tools.

\subsection{Literate computing}

Tools described in the previous section for literate programming are mature and
have been used to great effect to improve the quality of documentation in
scientific programs and data analysis, especially in the R community.  But they
remain firmly rooted in the original model proposed by Knuth, of authoring a
literate file that is then post-processed by various tools to produce either
documentation or executable code.  

In this section, we will contrast these with an alternate approach to the same
questions of improving the connection between code and documentation, that we
refer to as ``literate computing''.  Our choice of terminology points to the
fact that we emphasize the act of computing itself rather than the writing of
code, as all the systems we will describe are all centered around
\emph{interactive environments} where the user can enter code for immediate
execution, obtain results, and continue with more commands that produce new
results based on the previous ones.  A literate computing environment is one
that allows users not only to execute commands but also to store in a literate
document format the results of these commands along with figures and free-form
text that typically can include formatted mathematical expressions.  In
practice it can be seen as a blend of a command-line environment such as the
Unix shell with a word processor, since the resulting documents can be read
like text, but contain blocks of code that were executed by the underlying
computational system.

The earliest full-fledged implementation of these ideas is the graphical user
interface of the Mathematica \emph{Notebook} system which dates back to early
versions of Mathematica on the NeXT computer platform and took advantage of the
superior graphical capabilities of NeXT.  Today a number of other systems (both
open source and proprietary) provide similar capabilities; on the open-source
front we notably mention the Maxima \mref{maxima notebook} symbolic computing
package, the Sage \mref{sage} mathematical computing system, and the
interactive computing project IPython, on which we will focus the rest of our
discussion.

\subsection{IPython}\label{IPython}

IPython\footnote{\url{http://ipython.org}} is a system for interactive and
parallel computing that has become the \emph{de facto} standard environment for
scientific computing and data analysis in the Python programming language.  It
was created by F. PÃ©rez in in 2001 as an interactive command-line shell for
Python, and has evolved into a large collaborative open-source project with
contributions from a broad team of scientists \cite{PER-GRA:2007}.  IPython
started with the simple purpose of building a better interactive environment
for everyday computational work by a single researcher.  But as it grew into a
broader open source project, the team working on it (most of them practicing
scientists) learned many of the skills of the trade from open source software
developers.  It is precisely from this interplay between scientific research
and software development that we highlight here, as through contact with the
tools and culture of the open source world, we saw how in many respects this
community had outpaced scientists in its robust and rigorous use of
computational resources.  Through the years, we have thus led a dual life,
where the contact with this open source community continually informs and
enriches our approach to computational research and has helped us build IPython
into a better scientific tool.

In 2011, a web-based notebook was developed in IPython that connects to the
same interactive core as the original command-line shell, but does so using a
web browser as the user interface, automatically enabling either local or
remote use as the system running the web browser can be different from that
executing the code, with all communication happening over the network.
Fig.~\ref{fig:IPython-notebook} shows a typical notebook session with code,
text, mathematics and figures.

\begin{figure}
  \begin{centering}
    \includegraphics[width=3.2in]{fig/ipython-notebook-specgram.png}\par
  \end{centering}

  \caption{\label{fig:IPython-notebook}The web-based IPython Notebook combines
    explanatory text, mathematics, multimedia, code and the results from
    executing the code.}
\end{figure}

The driving idea behind the IPython Notebook is to enable researchers to move
fluidly between all the phases of the research life-cycle we described earlier.
If the environment where they conduct their exploratory research can also
support all subsequent stages of this cycle, and does so while smoothly
integrating with the version control and process practices we've previously
discussed, the likelihood that a final published result will be reproducible
increases significantly.  The Notebook system is designed around two central
ideas: (a) an openly specified protocol to control an interactive computational
engine, and (b) an equally open format to record these interactions between the
user and the computational engine, including the results produced by the
computations.

Before diving into the specifics of these two ideas, we note that the above
design is independent of the Python language: while IPython started its life as
a Python-specific project, the vision of the Notebook system is
language-agnostic.  First, while working in Python, users can mark entire code
blocks for execution via a separate language by using a special syntax on the
block's first line: a user can for example start a block \texttt{\%\%R},
\texttt{\%\%bash} or \texttt{\%\%ruby} and IPython will execute the entire
block with the respective system.  Second, an \emph{entire notebook} can be
executed in a different language if a remote engine (referred to as a
\emph{kernel}) exists that implements the interaction protocol.  As of this
writing, prototype kernels have been written in Ruby, JavaScript and R, and
their development continues.

The IPython architecture provides a way to capture, version control, re-execute
and convert into other output formats, any computational session.  Notebooks
can be shared with colleagues in their native form for re-execution or
converted into HTML, \LaTeX or PDF formats for reading and dissemination.  They
can be used in slideshow mode to give presentations that remain connected to a
live computation and can be exported into plain scripts for traditional
execution outside of the IPython framework.

The IPython protocol consists of messages in JSON (JavaScript Object Notation)
format that encode all actions that an interactive user can request of a
computational kernel, such as executing code, transferring data or sending
results, among many others.  While this protocol is implemented in IPython,
third-parties can independently implement it and provide new kernels that will
be able to interact with the notebook interface and clients.  The notebook file
format is a simple JSON data structure that contains a series of one or more
``worksheets'', each of which is a list of cells.  A cell can contain either
text or code, and code cells can also have the output corresponding to the
execution.  All sub-structures in the notebook format (the entire notebook, the
worksheets and the individual cells) have attached flexible metadata
containers; this metadata can be used by post-processing tools.  The file
format stores the communication protocol's payloads unmodified, so it can be
thought of as a structured and filtered log (since the user chooses what to
keep while working interactively) of the computation.  

The IPython project has taken elements pioneered by the Mathematica and Sage
notebooks and created a generic protocol and file format to control and record
literate computing sessions in any programming language.  This was a deliberate
choice in contrast to the literate programming approach: by providing a tool
that operates close to the live workflow of research computing (in contrast to
the batch-processing mode encouraged by classic literate programming tools),
the resulting documents are immediately reproducible sessions that can be
published in their own right or as companion materials to a more traditional
manuscript.  Given how IPython also includes support for parallel computing,
which we haven't discussed here in detail in the interest of conciseness, the
system provides an end-to-end environment for the creation of reproducible
research.

The real-world possibilities this offers were demonstrated during a
collaboration in 2012 between the IPython team, a microbiology team led by Rob
Knight from the University of Colorado and Greg Caporaso from the University of
Northern Arizona, and Justin Riley from MIT who created the StarCluster system
\mref{SC} for deployment and control of parallel resources on Amazon's EC2
cloud platform.  As part of an NIH-funded workshop to explore the future of
genomics data analysis in the cloud, this combined team collaborated on
creating a fully parallelized analysis comparing the predictive behavior of
different sizes and locations of gene sequence reads when reconstructing
phylogenetic trees.  The microbiologists had developed a serial prototype of
this idea using their Qiime libraries \mref{qiime}, but a large-scale analysis
with a full dataset would require roughly a month of CPU time on a single
workstation.  By locating the IPython Notebook server on Amazon cloud
instances, the entire team was able to log into a single instance and by
editing the code directly in the cloud, in a single day turn this prototype
into a set of production notebooks that would execute the analysis in parallel
using multiple Amazon servers.  Once the parallel was tested, it became evident
that there was not only an interesting example of using cloud technologies for
rapid development of research ideas but also a biologically relevant finding;
within a week the team had completed a more extensive run using 24 hours of
execution on 32 nodes and submitted a manuscript for publication \cite{RWM+12}.
This paper is now accompanied by all of the IPython notebooks that enable any
reader to fully reproduce our analysis, change parameters and question our
assumptions, without having to re-implement absolutely anything or be hampered
by lack of access to the code and data.  We have made available not only the
final notebooks, but also the Amazon Virtual Machine Images (data files that
represent a virtual computer on Amazon's cloud platform), so that the entire
analysis can literally be re-executed under identical conditions by anyone with
an Amazon account.

This example, anecdotal as it may be, indicates there is validity in the vision
we propose here: that by providing tools that encompass the entire cycle of
research, from exploration to large-scale parallel production and publication,
we can provide the scientific community with results that are immediately
accessible to others and reproducible, seeding the continued evolution of the
research processs.



We close this section by noting that the above tools are also playing a role in
the last stage of the cycle we outlined earlier, education.  We will increase
our chance that the next generation of scientists adopts improved
reproducibility practices if we educate them with the same tools that we use
for everyday research, and a couple of modern efforts that aim to bring
improved computational literacy to scientific research have adopted the IPython
notebook.  Software Carpentry\footnote{\url{http://software-carpentry.org}} is
a project funded by the Alfred P. Sloan Foundation and led by Greg Wilson at
the Mozilla Foundation whose motto is Richard Feynman's famous ``What I cannot
create, I do not understand.''.  They produce, with rigorous follow-up and
assesment, workshops aimed at working scientists (typically graduate students
and postdocs, but always open to broad audiences) and whose purpose is to
instill in them a collection of skills and best practices for effectively using
computing as a daily research tool.  The Software Carpentry workshops cover
topics ranging from the basics of the Unix shell to version control, Makefile
automation of processes and basics of scientific Python including data analysis
and visualization.  They have recently adopted the IPython Notebook as the base
system for teaching the scientific Python parts of their curricula, and provide
the IPython team with direct feedback on its strengths and weaknesses as an
educational tool.  In a similar vein, Josh Bloom from the astronomy department
at UC Berkeley has led for a number of years 3-day workshops on the use of
Python as a tool for scientific
computing\footnote{\url{http://pythonbootcamp.info}}.  These are open to the
entire cammpus community and followed by an optional for-credit seminar where
students laern more advanced skills for using Python as a research tool.
F. PÃ©rez and other members of the IPython team at UC Berkeley regularly lecture
in the bootcamp and courses, where the notebook is the means for delivery of
course materials and interactive lecturing.  While we have identified a number
of weaknesses and areas for improvement, we have also found this environment to
be markedly superior to all previous tools we had used in the past for teaching
in similar contexts.

In summary, IPython embodies the main ideas we've woven into our discussion: it
was born out of the interplay between everyday scientific research and open
source software development, and it tries to be a natural part of the entire
research cycle in order to make it more fluid, efficient and reproducible.


\section{Conclusion}\label{conclusion}

As we seek to learn how the open source practice can inform our scientific
work, we must recognize that the ideal of scientific reproducibility
is by necessity a reality of shades. We can see a gradation that goes
from a pure mathematical result whose proof should be accessible to
any person skilled in the necessary specialty, to one-of-a-kind experiments
such as the Large Hadron Collider or the Hubble Space Telescope, that
can not be reproduced in any realistic sense. At each point in this
spectrum, however, we can always find ways to improve our confidence
in the results: whether we re-analyze the same unique datasets with
independently developed packages run by separate groups or we re-acquire
partial sampling of critical data multiple times, we should never
completely renounce the ideals of reproducibility because of practical
ndifficulties.

Similarly, in computational research we also have certain areas where
complete reproducibility is more challenging than others. Some projects
require computations carried on the largest supercomputers on the
planet, and these are expensive resources that can not be arbitrarily
allocated for repeated executions of the same problem. Others may
require access to enormous datasets that can not easily be transferred
to the desktop of any researcher wishing to re-execute an analysis.
But again, alternatives exist: it should be possible to validate scaled
versions of the largest problems run independently, against scaled
specimens created on the supercomputers for this purpose, and
sub-sampled datasets can be used to collect at least validation statistics
that may be informative of the trust we place on the published analysis.

%\begin{itemize}
%\item Levels of reproducibility: replication, validation, reproduction,
%new construction.
%\item Conditions: clarity, transparency and trace-ability, predictability
%(which requires automation), communicability
%\end{itemize}
%
%While the mechanical reproduction of computational results is not a
%panacea in itself; the rigor, openness, culture of validation, collaboration and
%other aspects of science must become a routine part of our computational practices.

\ldots

This is a large and complex problem that requires changing the educational
process for new scientists, the incentive models for promotions and
rewards, the publication system, and more.


\subsection{Changing culture}

%Open{*}: software, access (Elsevier), education, review.
%
%Internet: interactions for humans, code and data
%
%\begin{itemize}
%\item Open source software
%
%\begin{itemize}
%\item development akin to scientific culture
%\item viable alternatives to proprietary software
%\item tools and lessons for improving the scientific process: Github
%\end{itemize}
%
%\item Open access
%
%\begin{itemize}
%\item \url{thecostofknowledge.org}: Elsevier boycott
%\item FRPAA House hearing on March 29th.
%\end{itemize}
%
%\item Open review
%Ref \cite{10.3389/fncom.2012.00018}.
%
%\item Open education
%
%\begin{itemize}
%\item MIT Open Courseware, Khan Academy\ldots
%\item Stanford CS 221 in fall 2011: \textasciitilde{}160,000 students.
%\item Spring 2012:
%
%\begin{itemize}
%\item Sebastian Thrun leaves Stanford: Udacity.
%\item Stanford: Coursera.
%\end{itemize}
%\item MITx, TED-Ed\ldots
%\end{itemize}
%\end{itemize}

\subsection{Incentive models}

%Science has become computational, but the incentive models of science
%are single-mindedly focused on paper-oriented publications that completely
%ignore the computational process. Papers are accepted 
%
%What role should journals play?
%
%\begin{itemize}
%\item Journals should mandate that upon paper \emph{approval} (but before
%actual publication and with said publication being conditioned on
%the author meeting this last condition), authors must expose their
%version control system to the public, and that the publicly available
%version can faithfully reproduce (within the limitations discussed
%above) the published results. This public version then becomes available
%for the scientific community not only for download, but also as a
%starting point for further contribution and development.
%
%\item supplemental material
%
%\item peer review
%
%\item new BioMed Central journal, Open Research Computation
%
% \item ongoing thematic series in Source Code for Biology and Medicine.
%\cite{neylon2012changing}
%
%\end{itemize}
%
%How will all this impact scientists?
%
%\begin{itemize}
%
%\item funding
%\item academic merit review
%
%\end{itemize}

\subsection{Education and training}

%\paragraph{ {\bf Workshops}}
%
%\begin{itemize}
%
%\item Software carpentry
%
%\ldots basic training
%
%\item Python
%
%\ldots use of git
%
%\end{itemize}
%
%\paragraph{ {\bf Courses}}
%
%\begin{itemize}
%
%\item Josh
%
%\item Randy
%
%\item Titus
%
%\end{itemize}
%
%
\section*{Acknowledgments}
We would like to thank \ldots
%\begin{itemize}
%\item members of the scientific Python community
%\item scientist from various labs
%\item Brian Granger
%\item John Hunter
%\item reviewers
%\end{itemize}

\bibliographystyle{plain}
\bibliography{ipython}

\end{document}
